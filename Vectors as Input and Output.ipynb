{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Importing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T03:13:33.844380Z","iopub.status.busy":"2022-07-15T03:13:33.844026Z","iopub.status.idle":"2022-07-15T03:13:33.853719Z","shell.execute_reply":"2022-07-15T03:13:33.852749Z","shell.execute_reply.started":"2022-07-15T03:13:33.844343Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.optim import Adamax\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_style('darkgrid')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Function to Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#f\n","def f(x):\n","    y = -5*x**3 + x**2 + x + 2\n","    return y\n","\n","#f'\n","def f_1(x):\n","    y = -15*x**2 + 2*x + 1\n","    return y\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Sampling from defined function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# number of samples\n","num = 3\n","# size of sample vector and input layer\n","len = 10\n","# using a linear offset to obtain the upper bound allows for the same step size for all sample vectors\n","offset = 10\n","step = offset/(len-1)\n","\n","def sample():\n","    l = np.random.rand()*-5\n","    u = l+offset\n","    new = np.linspace(l, u, len)\n","    return new\n","\n","new_sample = sample()\n","X_sample = np.expand_dims(new_sample, axis=0)\n","\n","# each row is a sample\n","for i in range(num-1):\n","    new_sample = [sample()]\n","    X_sample = np.append(X_sample, new_sample, axis=0)\n","\n","Y_sample =  f(X_sample)\n","\n","X_sample = torch.from_numpy(X_sample).float()\n","Y_sample = torch.from_numpy(Y_sample).float()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Defining the Model, Loss and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# len i/o nodes, i_len nodes per layer and ELU activation fn\n","i_len = 30\n","\n","model = nn.Sequential(\n","    nn.Linear(len, i_len),\n","    nn.ELU(),\n","    nn.Linear(i_len, i_len),\n","    nn.ELU(),\n","    nn.Linear(i_len, i_len),\n","    nn.ELU(),\n","    nn.Linear(i_len, len)\n","    )\n","\n","# mean squared error as loss fn\n","loss_fn = nn.MSELoss()\n","\n","# choose optimizer\n","optim = Adamax(model.parameters())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Graphing model accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot(l,u,t):\n","    \n","    # steps required to maintain same step value as samples used for training\n","    n_steps = ( (u-l)/step ) + 1\n","    \n","    # inform user if steps required is not a whole number\n","    if not n_steps.is_integer():\n","        print(f'Number of steps required is not an integer: {n_steps}')\n","    n_steps = int(n_steps)\n","\n","    # create array with plotting range\n","    X_plot = np.linspace(l,u,n_steps)\n","    print(X_plot)\n","    Y_plot = f(X_plot)\n","\n","    # create arrays to test model and compare outputs\n","    X_temp = torch.from_numpy(X_plot).float()\n","    Y_temp = torch.from_numpy(Y_plot).float()\n","\n","    Y_preds = model(X_temp)\n","    Y_pred_plot = torch.reshape(Y_preds, (-1,)).detach().numpy()\n","\n","    loss_plot = loss_fn(Y_preds, Y_temp)\n","    \n","    plt.plot(X_plot, Y_plot, X_plot, Y_pred_plot)\n","    plt.legend(['Function to Model','MLP Approximation'])\n","\n","    title = t + f\"Epoch {epoch}, Loss {loss_plot:.2E}\"\n","    plt.title(title)\n","    plt.show()\n","    plt.clf()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_epochs = 2501\n","#batch_size = num\n","\n","for epoch in range(n_epochs):\n","    y_pred = model(X_sample)\n","    loss = loss_fn(y_pred, Y_sample)\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()\n","    if not (epoch%500):\n","        l = -5\n","        plot(l,l+offset,'')\n","    #print(f'Finished epoch {epoch}, latest loss {loss}')\n","\n","plot(-10,10,'Plotting beyond training sample range, ')"]},{"cell_type":"markdown","metadata":{},"source":["Plotting first derivative using autograd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_f1(l,u,t):\n","    n_step = 1000\n","\n","    X_plot = np.linspace(l,u,n_step)\n","    Y_plot = f_1(X_plot)\n","\n","    #to obtain grads, requires_grad must be set to true\n","    X_temp = torch.linspace(l, u, steps=n_step, requires_grad=True).view(-1,1)\n","    Y_temp = torch.from_numpy(Y_plot).float().view(-1,1)\n","\n","    #change y preds to deriv\n","    Y_preds = model(X_temp)\n","    Y_pred_plot = torch.reshape(Y_preds, (-1,)).detach().numpy()\n","\n","    #differentiate output w.r.t. input, to grad_output order. Retaining graph allows the graph to be stored and not recalculated to be used subsequently.\n","    grads = torch.autograd.grad(outputs=Y_preds, inputs=X_temp, grad_outputs=torch.ones_like(Y_preds), retain_graph=True)[0]\n","    grad_plot = torch.reshape(grads, (-1,)).detach().numpy()\n","\n","    loss_plot = loss_fn(grads, Y_temp)\n","    \n","    plt.plot(X_plot, Y_plot, X_plot, grad_plot)\n","    title = t + f\"Epoch {epoch}, Loss {loss_plot:.2E}\"\n","    plt.title(title)\n","    plt.legend(['Function to Model','MLP Approximation'])\n","    plt.show()\n","    plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_f1(0,10,'First Derivatives Compared, ')\n","plot_f1(-15,15,'First Derivatives outside training range, ')"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"vscode":{"interpreter":{"hash":"b2bb01db43c92608afe528117be8348869a8bbfc2e823dee3781b388bf5fc44d"}}},"nbformat":4,"nbformat_minor":4}
