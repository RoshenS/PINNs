{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T03:13:33.844380Z",
     "iopub.status.busy": "2022-07-15T03:13:33.844026Z",
     "iopub.status.idle": "2022-07-15T03:13:33.853719Z",
     "shell.execute_reply": "2022-07-15T03:13:33.852749Z",
     "shell.execute_reply.started": "2022-07-15T03:13:33.844343Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.double)\n",
    "torch.set_default_device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.optim import Adam\n",
    "from torch.optim import LBFGS\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import plotly.express as px\n",
    "import dash\n",
    "from dash import Dash, dcc, html, Input, Output, State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(x,y,z,nx,ny,nz):\n",
    "    grid_arr = list()\n",
    "    sx,sy,sz = 0,0,0\n",
    "    if (nx == 1): sx = x\n",
    "    if (ny == 1): sy = y\n",
    "    if (nz == 1): sz = z\n",
    "    xs = torch.linspace(sx,x,nx)\n",
    "    ys = torch.linspace(sy,y,ny)\n",
    "    zs = torch.linspace(sz,z,nz)\n",
    "    for x in xs:\n",
    "        for y in ys:\n",
    "            for z in zs:\n",
    "                grid_arr.append((x.item(),y.item(),z.item()))\n",
    "    grid_arr = torch.tensor(grid_arr)\n",
    "    return grid_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physical System to Model: 3D Deformation, Linear Isotropic\n",
    "\n",
    "Training tensor within region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of x and y to be modelled\n",
    "x_range, x_int = 10, 10\n",
    "y_range, y_int = 10e-3, 3\n",
    "z_range, z_int = 10e-3, 3\n",
    "\n",
    "train = grid(x_range,y_range,z_range,x_int,y_int,z_int).requires_grad_(True)\n",
    "\n",
    "#DBC, Position and Displacment.\n",
    "# Fixing y & z in place at x = 0\n",
    "DBC_pos = grid(0,y_range,z_range,1,y_int,z_int)\n",
    "DBC_disp = grid(0,0,0,1,y_int,z_int)\n",
    "DBC_pos, DBC_disp = DBC_pos.requires_grad_(True),DBC_disp.requires_grad_(True)\n",
    "DBC = [DBC_pos,DBC_disp]\n",
    "\n",
    "\n",
    "#NBC, Scalar Traction Force, Normal Vector, and Position.\n",
    "T_hat = [[(6/((10e-3)**2)),0,0],\n",
    "         [0,0,0],\n",
    "         [0,0,0],\n",
    "         [0,0,0],\n",
    "         [0,0,0]]\n",
    "\n",
    "n_hat = [(1.0,0,0),\n",
    "         (0,1.0,0),\n",
    "         (0,1.0,0),\n",
    "         (0,0,1.0),\n",
    "         (0,0,1.0)]\n",
    "\n",
    "T_hat, n_hat = torch.tensor(T_hat),torch.tensor(n_hat)\n",
    "\n",
    "T_pos = [grid(x_range,y_range,z_range,1,y_int,z_int).requires_grad_(True)]\n",
    "T_pos.append(grid(x_range,0,z_range,x_int,1,z_int).requires_grad_(True))\n",
    "T_pos.append(grid(x_range,y_range,z_range,x_int,1,z_int).requires_grad_(True))\n",
    "T_pos.append(grid(x_range,y_range,0,x_int,y_int,1).requires_grad_(True))\n",
    "T_pos.append(grid(x_range,y_range,z_range,x_int,y_int,1).requires_grad_(True))\n",
    "\n",
    "NBC = [T_pos,n_hat,T_hat]\n",
    "\n",
    "# Physical Parameters, E = Youngs Modulus (Pa), v = Poisson's ratio (Typically [-1,0.5]).\n",
    "# Sclera\n",
    "p = dict(E=1.2e6, v=0.45)\n",
    "#Stiffness mat, 2D\n",
    "C_inv = [[1, -p['v'], -p['v'], 0, 0, 0],\n",
    "     [-p['v'], 1, -p['v'], 0, 0, 0],\n",
    "     [-p['v'], -p['v'], 1, 0, 0, 0],\n",
    "     [0, 0, 0, 1+p['v'], 0, 0],\n",
    "     [0, 0, 0, 0, 1+p['v'], 0],\n",
    "     [0, 0, 0, 0, 0, 1+p['v']]]\n",
    "C_inv = np.array(C_inv)\n",
    "C_inv = C_inv/p['E']\n",
    "C = np.linalg.inv(C_inv)\n",
    "C = torch.tensor(C).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "With Force (F), Area (A), Length (L), and Elastic Modulus (E), $\\delta = \\frac{\\sigma L}{E} = \\frac{\\frac{F}{A} L}{E}$\n",
    "\n",
    "$F = 37.5N, A = 625mm^2, L = 10m, E = 1.2 MPa, \\delta = 1.25mm$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comp(ax1,ax2):\n",
    "    plot_input = train.clone()\n",
    "    pred = model(plot_input)\n",
    "    \n",
    "    loss= loss_fn(model, plot_input)\n",
    "    pred_plot = pred.cpu().detach().numpy() + plot_input.cpu().detach().numpy()\n",
    "\n",
    "    plt.scatter(pred_plot[:,ax1].T,pred_plot[:,ax2].T,label='Deformed',marker='.')\n",
    "    plt.xlabel('Position')\n",
    "    plt.title(f\"Deformation, Loss: {loss:.2E}, Epoch {epoch}\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_num nodes, i_len nodes per layer and Tanh activation fn, wide and shallow (64x4)\n",
    "i_num = 2\n",
    "i_len = 64\n",
    "#params to take and approximate\n",
    "params_in = 3\n",
    "params_out = 3\n",
    "\n",
    "# fn to create model\n",
    "def pinn(input_size, output_size, num_layers, nodes_per_layer, activation):\n",
    "    layers = []\n",
    "    \n",
    "    # Input layer\n",
    "    layers.append(nn.Linear(input_size, nodes_per_layer))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for layer in range(num_layers):\n",
    "        layers.append(nn.Linear(nodes_per_layer, nodes_per_layer))\n",
    "        if layer == 0 and activation==Siren:\n",
    "            layers.append(activation(l=0))\n",
    "        else:\n",
    "            layers.append(activation())\n",
    "    \n",
    "    # Output layer\n",
    "    layers.append(nn.Linear(nodes_per_layer, output_size))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# custom trainable Tanh activation fn\n",
    "class TTanh(nn.Module):\n",
    "    def __init__(self, features=i_len):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.a = nn.Parameter(torch.randn(1, features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.a * x)\n",
    "\n",
    "#custom SIREN activation fn\n",
    "class Siren(nn.Module):\n",
    "    def __init__(self, features=i_len, l=1):\n",
    "        super(Siren, self).__init__()\n",
    "        self.features = features\n",
    "        # Learnable parameters for the phase and bias\n",
    "        if l: w = 1 \n",
    "        else: w = 30\n",
    "        n = torch.sqrt(torch.tensor(6.0/features))\n",
    "        self.a_weight = nn.Parameter((torch.rand(1, features) *2*n - n)*w)\n",
    "        self.a_bias = nn.Parameter(torch.randn(1, features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the periodic activation function\n",
    "        return torch.sin( self.a_weight*x + self.a_bias)\n",
    "\n",
    "# Model to be trained using only Adam\n",
    "model = pinn(params_in, params_out, i_num, i_len, TTanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function has the following components:\n",
    "\n",
    "Physics Loss:\n",
    "\n",
    "$\\mathcal{L}_{Phy} = \\frac{1}{N}\\sum[\\frac{\\partial \\sigma_{ij}}{\\partial x_{j}}]^2$\n",
    "\n",
    "NBC Loss:\n",
    "\n",
    "$\\mathcal{L}_{NBC} = \\frac{1}{N}\\sum[\\hat{T}.n^T - \\sigma]^2$\n",
    "\n",
    "DBC Loss:\n",
    "\n",
    "$\\mathcal{L}_{DBC} = \\frac{1}{N}\\sum[u-\\hat{u}]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stress(model,input):\n",
    "    # Strain from displacement field\n",
    "    pred = model(input)\n",
    "    \n",
    "    dux = torch.autograd.grad(pred[:,0], input, torch.ones_like(pred[:,0]), retain_graph=True, create_graph=True)[0]\n",
    "    duy = torch.autograd.grad(pred[:,1], input, torch.ones_like(pred[:,1]), retain_graph=True, create_graph=True)[0]\n",
    "    duz = torch.autograd.grad(pred[:,2], input, torch.ones_like(pred[:,2]), retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "    duxdx = dux[:,0]\n",
    "    duxdy = dux[:,1]\n",
    "    duxdz = dux[:,2]\n",
    "\n",
    "    duydx = duy[:,0]\n",
    "    duydy = duy[:,1]\n",
    "    duydz = duy[:,2]\n",
    "\n",
    "    duzdx = duz[:,0]\n",
    "    duzdy = duz[:,1]\n",
    "    duzdz = duz[:,2]\n",
    "\n",
    "    epsilon = torch.stack((duxdx, duydy, duzdz, 0.5*(duxdy + duydx), 0.5*(duxdz + duzdx), 0.5*(duydz + duzdy)))\n",
    "    \n",
    "    # Stress from Strain and Constitutive matrix\n",
    "    sigma_6xn = C @ epsilon\n",
    "\n",
    "    return sigma_6xn[0,:],sigma_6xn[1,:],sigma_6xn[2,:],sigma_6xn[3,:],sigma_6xn[4,:],sigma_6xn[5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, batch):\n",
    "    # model: model\n",
    "    # batch: training batch\n",
    "\n",
    "    # Calulate loss from DBC\n",
    "    DBC_loss = ((model(DBC[0])-DBC[1])**2).mean()\n",
    "\n",
    "\n",
    "    #Calculating NBC Loss\n",
    "    NBC_loss = 0\n",
    "    for i in range(len(NBC[0])):\n",
    "        sxx_NBC,syy_NBC,szz_NBC,sxy_NBC,sxz_NBC,syz_NBC = Stress(model,NBC[0][i])\n",
    "        syx_NBC,szx_NBC,szy_NBC = sxy_NBC.clone(),sxz_NBC.clone(),syz_NBC.clone()\n",
    "\n",
    "        sigma_1x3_x_NBC = torch.stack((sxx_NBC,syx_NBC,szx_NBC), dim=1)\n",
    "        sigma_1x3_y_NBC = torch.stack((sxy_NBC,syy_NBC,szy_NBC), dim=1)\n",
    "        sigma_1x3_z_NBC = torch.stack((sxz_NBC,syz_NBC,szz_NBC), dim=1)\n",
    "        sigma_3x3_NBC = torch.stack((sigma_1x3_x_NBC, sigma_1x3_y_NBC, sigma_1x3_z_NBC), dim=1)\n",
    "\n",
    "        #test\n",
    "        prev_sigma.append(sigma_3x3_NBC.detach())\n",
    "\n",
    "        #NBC Loss\n",
    "        T = torch.matmul(NBC[1][i],sigma_3x3_NBC)\n",
    "        NBC_loss += ((T - NBC[2][i])**2).mean()\n",
    "\n",
    "\n",
    "    #Physics Loss\n",
    "    #Stress\n",
    "    sxx_phy,syy_phy,szz_phy,sxy_phy,sxz_phy,syz_phy = Stress(model,batch)\n",
    "\n",
    "    #Divergence of Stress\n",
    "    dsxx = torch.autograd.grad(sxx_phy, batch, torch.ones_like(sxx_phy), retain_graph=True, create_graph=True)[0]\n",
    "    dsyy = torch.autograd.grad(syy_phy, batch, torch.ones_like(syy_phy), retain_graph=True, create_graph=True)[0]\n",
    "    dszz = torch.autograd.grad(szz_phy, batch, torch.ones_like(szz_phy), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    dsxy = torch.autograd.grad(sxy_phy, batch, torch.ones_like(sxy_phy), retain_graph=True, create_graph=True)[0]\n",
    "    dsxz = torch.autograd.grad(sxz_phy, batch, torch.ones_like(sxz_phy), retain_graph=True, create_graph=True)[0]\n",
    "    dsyz = torch.autograd.grad(syz_phy, batch, torch.ones_like(syz_phy), retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "\n",
    "    dsxxdx = dsxx[:,0]\n",
    "    dsyydy = dsyy[:,1]\n",
    "    dszzdz = dszz[:,2]\n",
    "\n",
    "    dsxydx = dsxy[:,0]\n",
    "    dsxydy = dsxy[:,1]\n",
    "\n",
    "    dsxzdx = dsxz[:,0]\n",
    "    dsxzdz = dsxz[:,2]\n",
    "\n",
    "    dsyzdy = dsyz[:,1]\n",
    "    dsyzdz = dsyz[:,2]\n",
    "\n",
    "    div_stress = torch.stack((dsxxdx + dsxydy + dsxzdz,\n",
    "                              dsxydx + dsyydy + dsyzdz,\n",
    "                              dsxzdx + dsyzdy + dszzdz), dim=0)\n",
    "    \n",
    "    #test\n",
    "    prev_div.append(div_stress.detach())\n",
    "    \n",
    "    phy_loss = ((div_stress)**2).mean()\n",
    "\n",
    "    loss = DBC_loss + NBC_loss + phy_loss\n",
    "\n",
    "    prev_loss.append(loss.item())\n",
    "    prev_NBC.append(NBC_loss.item())\n",
    "    prev_DBC.append(DBC_loss.item())\n",
    "    prev_phy.append(phy_loss.item())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_adam = {'epoch':-1}\n",
    "best_model_LBFGS = {'epoch':-1}\n",
    "best_model = {'epoch':-1}\n",
    "\n",
    "def checkpoint(best_model):\n",
    "    if (best_model['epoch'] == -1) or (prev_loss[-1] < best_model['loss']):\n",
    "        best_model = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': copy.deepcopy(copy.deepcopy(model.state_dict())),\n",
    "            'optimizer_state_dict': copy.deepcopy(copy.deepcopy(optim.state_dict())),\n",
    "            'loss': prev_loss[-1],\n",
    "            'model_params':[params_in, params_out, i_num, i_len],\n",
    "        'p_sigma':prev_sigma[-1],\n",
    "        'p_div':prev_div[-1]\n",
    "                    }\n",
    "    return best_model\n",
    "\n",
    "def final(model):\n",
    "    model = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': copy.deepcopy(copy.deepcopy(model.state_dict())),\n",
    "        'optimizer_state_dict': copy.deepcopy(copy.deepcopy(optim.state_dict())),\n",
    "        'loss': prev_loss[-1],\n",
    "        'model_params':[params_in, params_out, i_num, i_len],\n",
    "        'p_sigma':prev_sigma[-1],\n",
    "        'p_div':prev_div[-1]\n",
    "                }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(t=\"\"):\n",
    "    plt.plot(np.log10(prev_loss),c='tab:blue',label='Log Loss')\n",
    "    plt.title(f\"Log Loss over {len(prev_loss)} Calls\"+t)\n",
    "    plt.xlabel(\"Calls\")\n",
    "    plt.ylabel(\"Log Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(np.log10(prev_phy),label='Log Physics Loss',c='tab:green')\n",
    "    plt.title(f\"Physics Loss over {len(prev_phy)} Calls\"+t)\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log Physics Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(np.log10(prev_NBC),label='Log NBC Loss',c='tab:orange')\n",
    "    plt.title(f\"NBC Loss over {len(prev_NBC)} Calls\"+t)\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log NBC Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(np.log10(prev_DBC),label='Log DBC Loss',c='tab:red')\n",
    "    plt.title(f\"DBC Loss over {len(prev_DBC)} Calls\"+t)\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log DBC Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closure Fn for Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    loss = loss_fn(model, train)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "n_epochs = (15000,500)\n",
    "w_update = 1\n",
    "epochs = 0\n",
    "prev_loss, prev_phy, prev_NBC, prev_DBC = list(),list(),list(),list()\n",
    "training = True\n",
    "\n",
    "#test\n",
    "prev_div, prev_sigma = list(),list()\n",
    "\n",
    "while (training):\n",
    "    # start with Adam optimizer\n",
    "    optim = Adam(model.parameters())\n",
    "    for epoch in range(n_epochs[0]):\n",
    "        \n",
    "        optim.step(closure)\n",
    "        best_model_adam = checkpoint(best_model_adam)\n",
    "        best_model = checkpoint(best_model)\n",
    "\n",
    "        if not (epoch%1000): print(f'Finished epoch {epochs+epoch}, latest loss {prev_loss[-1]:.2E}')\n",
    "    \n",
    "    final_model_adam = final(model)\n",
    "    plot_train(\" with Adam\")\n",
    "    epochs += n_epochs[0]\n",
    "\n",
    "    # continue with LBFGS\n",
    "    optim = LBFGS(model.parameters())\n",
    "    for epoch in range(n_epochs[1]):\n",
    "        optim.step(closure)\n",
    "\n",
    "        # reset if loss becomes NAN\n",
    "        if np.isnan(prev_loss[-1]):\n",
    "            optim = LBFGS(model.parameters())\n",
    "            model.load_state_dict(best_model_LBFGS['model_state_dict'])\n",
    "            optim.load_state_dict(best_model_LBFGS['optimizer_state_dict'])\n",
    "            loss = best_model_LBFGS['loss']\n",
    "\n",
    "        best_model_LBFGS = checkpoint(best_model_LBFGS)\n",
    "        best_model = checkpoint(best_model)\n",
    "\n",
    "        if not (epoch%100): print(f'Finished epoch {epochs+epoch}, latest loss {prev_loss[-1]:.2E}')\n",
    "    \n",
    "    final_model_LBGFS = final(model)\n",
    "    plot_train(\" with LBGFS\")\n",
    "    epochs += n_epochs[1]\n",
    "\n",
    "\n",
    "    # User determines if training should continue\n",
    "    user_input = input(\"Enter epochs to continue training (Adam LBGFS) or 0 to stop:\")\n",
    "    try:\n",
    "        n_epochs = tuple(int(item) for item in user_input.split())\n",
    "        print(n_epochs)\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        n_epochs = (0,0)\n",
    "    if len(n_epochs) != 2: training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model['model_state_dict'])\n",
    "optim.load_state_dict(best_model['optimizer_state_dict'])\n",
    "epoch = best_model['epoch']\n",
    "loss = best_model['loss']\n",
    "model.eval()\n",
    "print(final_model_adam['p_sigma'])\n",
    "print(final_model_adam['p_div'])\n",
    "\n",
    "plot_comp(0,1)\n",
    "plot_comp(0,2)\n",
    "plot_comp(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = f\"3D_Isotropic_Beam_Model_{loss:.2E}_{epoch}.tar\"\n",
    "#torch.save(best_model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"3D_Isotropic_Beam_Model_6.66E+00_999.tar\"\n",
    "\n",
    "#prevpoint = torch.load(PATH, map_location=device)\n",
    "#model.load_state_dict(prevpoint['model_state_dict'])\n",
    "#optim.load_state_dict(prevpoint['optimizer_state_dict'])\n",
    "#epoch = prevpoint['epoch']\n",
    "#loss = prevpoint['loss']\n",
    "#model = model.to(device)\n",
    "#model.eval()\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploty and Dash Animaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting = grid(x_range,y_range,z_range,x_int,y_int,z_int).requires_grad_(True)\n",
    "max_disp, max_x, max_y, max_z = 0,0,0,0\n",
    "\n",
    "# obtaining plot data\n",
    "model_pred = list()\n",
    "for i in range(len(plotting)):\n",
    "    #x,y,z,dx,dy,dz\n",
    "    pred = model(plotting[i])\n",
    "    model_pred.append([plotting[i][0].item(), plotting[i][1].item(), plotting[i][2].item(), pred[:,0].item(), pred[:,1].item(), pred[:,2].item()])\n",
    "    if (np.sqrt(pred[:,0].item()**2 + pred[:,1].item()**2 + pred[:,2].item()**2) > max_disp): max_disp = np.sqrt(pred[:,0].item()**2 + pred[:,1].item()**2 + pred[:,2].item()**2)\n",
    "    if (np.abs(pred[:,0].item()) > max_x): max_x = pred[:,0].item()\n",
    "    if (np.abs(pred[:,1].item()) > max_y): max_y = pred[:,1].item()\n",
    "    if (np.abs(pred[:,2].item()) > max_z): max_z = pred[:,2].item()\n",
    "model_pred = np.array(model_pred)\n",
    "\n",
    "print(f\"max x = {max_x}\")\n",
    "print(f\"max y = {max_y}\")\n",
    "print(f\"max z = {max_z}\")\n",
    "\n",
    "#setting limits\n",
    "init = model_pred[:,0:3]\n",
    "disp = model_pred[:,0:3] + model_pred[:,3:]\n",
    "buffer = 0.4\n",
    "\n",
    "init_scatter = go.Scatter3d(\n",
    "    x=model_pred[:, 0],\n",
    "    y=model_pred[:, 1],\n",
    "    z=model_pred[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        colorscale='RdYlGn_r',\n",
    "        color=np.zeros(len(model_pred[:, 0])),\n",
    "        cmin=0,\n",
    "        cmax=1,\n",
    "        colorbar=dict(title='Normalized Displacement'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create init scatter\n",
    "fig = go.Figure(data=[init_scatter])\n",
    "\n",
    "# Create a Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define the app layout\n",
    "app.layout = html.Div([\n",
    "    html.H1('Displacement Animation', style={'color': 'white'}),\n",
    "    \n",
    "    dcc.Graph(id='scatter-plot', figure=fig),\n",
    "    \n",
    "    dcc.Slider(\n",
    "        id=\"frame-slider\",\n",
    "        min=0,\n",
    "        max=100,\n",
    "        step=0.1,\n",
    "        value=0,\n",
    "        marks={i: str(i) for i in range(0, 101, 10)},\n",
    "        tooltip={\"placement\": \"bottom\"},\n",
    "        updatemode='drag'\n",
    "    )\n",
    "])\n",
    "\n",
    "# Callback to update the scatter plot\n",
    "@app.callback(\n",
    "    Output('scatter-plot', 'figure'),\n",
    "    [Input('frame-slider', 'value')]\n",
    ")\n",
    "\n",
    "def update_scatter_plot(frame):\n",
    "    # Update only the color data of the scatter plot\n",
    "    fig.data[0].x = model_pred[:, 0] + frame * model_pred[:, 3] / 100\n",
    "    fig.data[0].y =model_pred[:, 1] + frame * model_pred[:, 4] / 100\n",
    "    fig.data[0].z =model_pred[:, 2] + frame * model_pred[:, 5] / 100\n",
    "    fig.data[0].marker.color = np.sqrt(model_pred[:, 3]**2 + model_pred[:, 4]**2 + model_pred[:, 5]**2) * frame / max_disp / 100\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(range=[np.min(np.vstack([init, disp])) - buffer, np.max(np.vstack([init, disp])) + buffer]),\n",
    "            yaxis=dict(range=[np.min(np.vstack([init, disp])) - buffer, np.max(np.vstack([init, disp])) + buffer]),\n",
    "            zaxis=dict(range=[np.min(np.vstack([init, disp])) - buffer, np.max(np.vstack([init, disp])) + buffer]),\n",
    "        ),\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        uirevision='locked',\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2bb01db43c92608afe528117be8348869a8bbfc2e823dee3781b388bf5fc44d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
