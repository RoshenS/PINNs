{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Importing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-15T03:13:33.844380Z","iopub.status.busy":"2022-07-15T03:13:33.844026Z","iopub.status.idle":"2022-07-15T03:13:33.853719Z","shell.execute_reply":"2022-07-15T03:13:33.852749Z","shell.execute_reply.started":"2022-07-15T03:13:33.844343Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.optim import Adam\n","from torch.optim import LBFGS\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from scipy import integrate # numerical integration tool for ode's"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Physical System to Model: Spring, Mass, and Damper System\n","\n","$ F(t) -c \\frac{d\\mathbf{x}}{d t}(t) - kx(t) = m\\frac{d\\mathbf{x^2}}{d t}(t)$\n","\n","$\\frac{d\\mathbf{x^2}}{d t}(t) = ( F(t) -c \\frac{d\\mathbf{x}}{d t}(t) - kx(t) ) /m$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Physical Parameters, c = damping constant, k =  stiffness of spring, m = mass.\n","p = dict(c=4, k=2, m=10, F=0, x0=1, dx0dt=0)\n","p['d2x0dt2'] = (p['F'] - p['c']*p['dx0dt'] - p['k']*p['x0']) / p['m']\n","#initial conditions\n","x0 = [p['x0'],p['dx0dt']]\n","#plot to time T\n","T = 30\n","\n","def f(x,t,p):\n","    x1, x2 = x[0], x[1]\n","    dx1 = x2\n","    dx2 = (p['F'] - p['c']*x2 - p['k']*x1) / p['m']\n","    return([dx1,dx2])\n","\n","#Function to solve the physical system using numerical integration tool\n","def run_oscil(x0,T,p):\n","    t = np.linspace(0, T, 1000)\n","    dxdt = lambda x,t : f(x, t, p)\n","    x_t = integrate.odeint(dxdt, x0, t)\n","    return(t,x_t)\n","\n","#Plotting neumerical solution\n","t_plot, x_plot = run_oscil(x0,T,p)\n","plt.plot(t_plot,x_plot)\n","plt.xlabel('time [s]')\n","#x_1 is position, x_2 is velocity\n","plt.legend(['$x_1(t)$','$x_2(t)$'])\n","plt.title(\"Numerical Solution to Spring, Mass, and Damper system\")\n"]},{"cell_type":"markdown","metadata":{},"source":["The equation that models the Spring, Mass, and Damper System is the following:\n","$ F(t) = m\\frac{d\\mathbf{x^2}}{d t}(t) + c \\frac{d\\mathbf{x}}{d t}(t) + kx(t)$\n","\n","The following parameters are needed to fully define the system:\n","1. Sum of Forces (F)\n","2. Mass (m)\n","3. Damping Constant (c)\n","4. Spring Stiffness (k)\n","5. Time (t)\n","6. Initial Conditions (x0)\n","\n","These parameters can be used to determine the position (x) at current time (t)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Creating vectors for all parameters to be modelled"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_tuple = tuple()\n","# Range of t to be modelled [0,30]S\n","t_train = torch.linspace(0,30,64).view(-1,1)\n","train_tuple += (t_train,)\n","\n","#setting other variables as constants\n","F_train = torch.ones_like(t_train)*p['F']\n","#train_tuple += (F_train,)\n","m_train = torch.ones_like(t_train)*p['m']\n","#train_tuple += (m_train,)\n","c_train = torch.ones_like(t_train)*p['c']\n","#train_tuple += (c_train,)\n","k_train = torch.ones_like(t_train)*p['k']\n","#train_tuple += (k_train,)\n","x0_train = torch.ones_like(t_train)*p['x0']\n","#train_tuple += (x0_train,)\n","dx0dt_train = torch.ones_like(t_train)*p['dx0dt']\n","#train_tuple += (dx0dt_train,)\n","d2x0dt2_train = torch.ones_like(t_train)*p['d2x0dt2']\n","#train_tuple += (d2x0dt2_train,)\n","\n","train = torch.cat(train_tuple,1).requires_grad_(True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Defining the Model, Loss and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# len i/o nodes, i_len nodes per layer and Tanh activation fn, wide and shallow (64x4)\n","i_len = 64\n","\n","#params to take and approximate\n","params_in = 1\n","params_out = 1\n","\n","model = nn.Sequential(\n","    nn.Linear(params_in, i_len),\n","    \n","    nn.Tanh(),\n","    nn.Linear(i_len, i_len),\n","    nn.Tanh(),\n","    nn.Linear(i_len, i_len),\n","    nn.Tanh(),\n","\n","    nn.Linear(i_len, params_out)\n","    )\n","\n","# choose optimizer\n","optim = LBFGS(model.parameters(),history_size=15)    "]},{"cell_type":"markdown","metadata":{},"source":["Creating a custom loss function using the following:\n","\n","Physics Loss:\n","\n","$ F(t) - m\\frac{d\\mathbf{x^2}}{d t}(t) - c \\frac{d\\mathbf{x}}{d t}(t) - kx(t) = 0$\n","\n","Initial Conditions Loss:\n","\n","$x(0) = x_0$\n","\n","$\\frac{d\\mathbf{x}}{d t}(0) = \\frac{d\\mathbf{x_0}}{d t}(t)$\n","\n","$\\frac{d\\mathbf{x^2}}{d t}(0) = ( F(t) -c \\frac{d\\mathbf{x}}{d t}(0) - kx(0) ) /m$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def loss_fn(model, inputs):\n","    # model: model\n","    # batch: vector with cols 0:t, 1:F, 2:M, 3:c, 4:k, 5:x(0), 6:x'(0), 7:x''(0)\n","\n","    # Obtaining parameters to compute loss.\n","    x_pred = model(inputs)\n","\n","    first_deriv = torch.autograd.grad(outputs=x_pred, inputs=inputs, grad_outputs=torch.ones_like(x_pred), create_graph=True, retain_graph=True)[0]\n","    second_deriv = torch.autograd.grad(outputs=first_deriv, inputs=inputs, grad_outputs=torch.ones_like(first_deriv), retain_graph=True)[0]\n","    dxdt_pred = first_deriv[:, 0]\n","    d2xdt2_pred = second_deriv[:, 0]\n","\n","    F = p['F']\n","    #F = inputs[:,1]\n","    m = p['m']\n","    #m = inputs[:,2]\n","    c = p['c']\n","    #c = inputs[:,3]\n","    k = p['k']\n","    #k = inputs[:,4]\n","\n","    # Calculate loss based on governing physics eqns\n","    phy_loss = torch.mean( (F - m*d2xdt2_pred - c*dxdt_pred - k*x_pred)**2 )\n","\n","    # Calulate loss from boundary conditions\n","    initial_input = inputs.detach().clone()\n","    initial_input[:,0] = torch.zeros_like(boundary_input[:,0])\n","    initial_input.requires_grad_(True)\n","    \n","    x0_pred = model(initial_input)\n","    dx0dt_pred = torch.autograd.grad(x0_pred, initial_input, torch.ones_like(x0_pred), create_graph=True, retain_graph=True)[0]\n","    d2x0dt2_pred = torch.autograd.grad(dx0dt_pred, initial_input, torch.ones_like(dx0dt_pred), retain_graph=True)[0]\n","\n","    x0 = p['x0']\n","    #x0 = initial_input[:,5]\n","    dx0dt = p['dx0dt']\n","    #dx0dt = initial_input[:,6]\n","    d2x0dt2 = p['d2x0dt2']\n","    #d2x0dt2 = initial_input[:,7]\n","\n","    x0_loss = torch.mean( (x0_pred.view(-1,1) - x0)**2 )\n","    dx0dt_loss = torch.mean( (dx0dt_pred.view(-1,1) - dx0dt)**2 )\n","    d2x0dt2_loss = torch.mean( (d2x0dt2_pred.view(-1,1) - d2x0dt2)**2 )\n","    i_loss = x0_loss + dx0dt_loss + d2x0dt2_loss\n","\n","    loss = phy_loss + i_loss\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["Train..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_epochs = 3000\n","prev_loss = list()\n","\n","def closure():\n","    loss = loss_fn(model, train)\n","    optim.zero_grad()\n","    loss.backward()\n","    prev_loss.append(loss.item())\n","    return loss\n","\n","for epoch in range(n_epochs):\n","    optim.step(closure)\n","    if not (epoch%100):\n","        print(f'Finished epoch {epoch}, latest loss {prev_loss[-1]:.2E}')\n","        prev_loss = list()"]},{"cell_type":"markdown","metadata":{},"source":["Comparing plots of MLP approximation against numerical solution."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(t_plot,x_plot[:,0],label='Numerical Solution')\n","plt.xlabel('time [s]')\n","plt.title(\"Comparing MLP Approximation and Numerical Solution\")\n","\n","# obtaining plotting information\n","# inputs: vector with cols 0:t, 1:F, 2:M, 3:c, 4:k, 5:x(0), 6:x'(0)\n","\n","#creating tensors to be plotted\n","plot_tuple = tuple()\n","\n","plot_t = torch.from_numpy(t_plot).float().view(-1,1)\n","plot_tuple += (plot_t,)\n","plot_F = torch.ones_like(plot_t)*p['F']\n","#plot_tuple += (plot_F,)\n","plot_m = torch.ones_like(plot_t)*p['m']\n","#plot_tuple += (plot_m,)\n","plot_c = torch.ones_like(plot_t)*p['c']\n","#plot_tuple += (plot_c,)\n","plot_k = torch.ones_like(plot_t)*p['k']\n","#plot_tuple += (plot_k,)\n","plot_x0 = torch.ones_like(plot_t)*p['x0']\n","#plot_tuple += (plot_x0,)\n","plot_dx0dt = torch.ones_like(plot_t)*p['dx0dt']\n","#plot_tuple += (plot_dx0dt,)\n","plot_d2x0dt2 = torch.ones_like(plot_t)*p['d2x0dt2']\n","#plot_tuple += (plot_d2x0dt2,)\n","\n","plot_input = torch.cat(plot_tuple, 1)\n","pred = model(plot_input)\n","pred_plot = torch.reshape(pred, (-1,)).detach().numpy()\n","\n","plt.plot(t_plot,pred_plot,label='MLP Approximation')\n","plt.legend()"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"vscode":{"interpreter":{"hash":"b2bb01db43c92608afe528117be8348869a8bbfc2e823dee3781b388bf5fc44d"}}},"nbformat":4,"nbformat_minor":4}
