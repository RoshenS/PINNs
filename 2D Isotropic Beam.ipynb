{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T03:13:33.844380Z",
     "iopub.status.busy": "2022-07-15T03:13:33.844026Z",
     "iopub.status.idle": "2022-07-15T03:13:33.853719Z",
     "shell.execute_reply": "2022-07-15T03:13:33.852749Z",
     "shell.execute_reply.started": "2022-07-15T03:13:33.844343Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim import LBFGS\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physical System to Model: 2D Deformation, Linear Isotropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBC, positions and displacments\n",
    "pos = [(0.0,0.0)]\n",
    "disp = [(0.0,0.0)]\n",
    "pos, disp = torch.tensor(pos).requires_grad_(True),torch.tensor(disp).requires_grad_(True)\n",
    "pos, disp = pos.to(device),disp.to(device)\n",
    "DBC = [pos,disp]\n",
    "\n",
    "#NBC, Scalar Traction Force, Normal Vector, and Position.\n",
    "T_hat = [(2.0,0)]\n",
    "n_hat = [(1.0,0)]\n",
    "T_pos = [(10.0,5.0)]\n",
    "T_hat, n_hat, T_pos = torch.tensor(T_hat).requires_grad_(True).t(),torch.tensor(n_hat).requires_grad_(True).t(),torch.tensor(T_pos).requires_grad_(True)\n",
    "T_hat, n_hat, T_pos = T_hat.to(device),n_hat.to(device),T_pos.to(device)\n",
    "NBC = [T_hat,n_hat,T_pos]\n",
    "\n",
    "\n",
    "# Physical Parameters, E = Youngs Modulus, v = Poisson's ratio.\n",
    "# Rubber\n",
    "p = dict(E=4.0, v=0.45)\n",
    "#Stiffness mat, 2D\n",
    "C = [[1, -p['v'], 0],\n",
    "     [-p['v'], 1, 0],\n",
    "     [0, 0, 1+p['v']]]\n",
    "C = torch.tensor(C).requires_grad_(True)\n",
    "C = C/p['E']\n",
    "C = C.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating vector for Training Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "# Range of t to be modelled [0,10]\n",
    "combi = torch.linspace(0,m,20).requires_grad_(True)\n",
    "train = torch.combinations(combi, with_replacement=True)\n",
    "train = train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comp():\n",
    "    x_plot = np.linspace(0, m, 5)\n",
    "    y_plot = np.linspace(0, m, 5)\n",
    "\n",
    "    combi = torch.linspace(0,m,5).requires_grad_(True)\n",
    "    plot_input = torch.combinations(combi, with_replacement=True)\n",
    "    plot_input = train.to(device)\n",
    "    pred = model(plot_input)\n",
    "    \n",
    "    _, loss_absolute = loss_fn(model, plot_input)\n",
    "    pred_plot = pred.cpu().detach().numpy() + plot_input.cpu().detach().numpy()\n",
    "\n",
    "    plt.plot(pred_plot[:,0],pred_plot[:,1],label='Deformed',marker='.')\n",
    "    plt.xlabel('Position')\n",
    "    plt.title(f\"Deformation, Loss: {loss_absolute:.2E}, Epoch {epoch}\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "  (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (2): TTanh()\n",
       "  (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (4): TTanh()\n",
       "  (5): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i_num nodes, i_len nodes per layer and Tanh activation fn, wide and shallow (64x4)\n",
    "i_num = 2\n",
    "i_len = 64\n",
    "#params to take and approximate\n",
    "params_in = 2\n",
    "params_out = 2\n",
    "\n",
    "# fn to create model\n",
    "def pinn(input_size, output_size, num_layers, nodes_per_layer, activation):\n",
    "    layers = []\n",
    "    \n",
    "    # Input layer\n",
    "    layers.append(nn.Linear(input_size, nodes_per_layer))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for layer in range(num_layers):\n",
    "        layers.append(nn.Linear(nodes_per_layer, nodes_per_layer))\n",
    "        if layer == 0 and activation==Siren:\n",
    "            layers.append(activation(l=0))\n",
    "        else:\n",
    "            layers.append(activation())\n",
    "    \n",
    "    # Output layer\n",
    "    layers.append(nn.Linear(nodes_per_layer, output_size))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# custom trainable Tanh activation fn\n",
    "class TTanh(nn.Module):\n",
    "    def __init__(self, features=i_len):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.a = nn.Parameter(torch.randn(1, features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.a * x)\n",
    "\n",
    "#custom SIREN activation fn\n",
    "class Siren(nn.Module):\n",
    "    def __init__(self, features=i_len, l=1):\n",
    "        super(Siren, self).__init__()\n",
    "        self.features = features\n",
    "        # Learnable parameters for the phase and bias\n",
    "        if l: w = 1 \n",
    "        else: w = 30\n",
    "        n = torch.sqrt(torch.tensor(6.0/features))\n",
    "        self.a_weight = nn.Parameter((torch.rand(1, features) *2*n - n)*w)\n",
    "        self.a_bias = nn.Parameter(torch.randn(1, features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the periodic activation function\n",
    "        return torch.sin( self.a_weight*x + self.a_bias)\n",
    "\n",
    "# Model to be trained using only Adam\n",
    "model = pinn(params_in, params_out, i_num, i_len, TTanh)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function has the following components:\n",
    "\n",
    "Physics Loss:\n",
    "\n",
    "$\\mathcal{L}_{Phy} = \\frac{1}{N}\\sum[\\frac{\\partial \\sigma_{ij}}{\\partial x_{j}}]^2$\n",
    "\n",
    "NBC Loss:\n",
    "\n",
    "$\\mathcal{L}_{NBC} = \\frac{1}{N}\\sum[\\sigma.n - \\hat{T}]^2$\n",
    "\n",
    "DBC Loss:\n",
    "\n",
    "$\\mathcal{L}_{DBC} = \\frac{1}{N}\\sum[u-\\hat{u}]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stress(model,input):\n",
    "    # Strain from displacement field\n",
    "    pred = model(input)\n",
    "    \n",
    "    dux = torch.autograd.grad(pred[:,0], input, torch.ones_like(pred[:,0]), retain_graph=True, create_graph=True)[0]\n",
    "    duy = torch.autograd.grad(pred[:,1], input, torch.ones_like(pred[:,1]), retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "    duxdx = dux[:,0]\n",
    "    duxdy = dux[:,1]\n",
    "    duydx = duy[:,0]\n",
    "    duydy = duy[:,1]\n",
    "    \n",
    "    epsilon = torch.zeros(3,len(duxdx)).requires_grad_(True)\n",
    "    epsilon = epsilon.to(device)\n",
    "    epsilon[0,:],epsilon[1,:],epsilon[2,:] = duxdx, duydy, 0.5 * (duydx + duxdy)\n",
    "    \n",
    "    # Stress from Strain and Constitutive matrix\n",
    "    sigma_3xn = C @ epsilon\n",
    "    \n",
    "    return sigma_3xn[0,:],sigma_3xn[1,:],sigma_3xn[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, batch):\n",
    "    # model: model\n",
    "    # batch: vector with x\n",
    "\n",
    "    # Calulate loss from initial conditions (DBC)\n",
    "    DBC_loss = ((model(DBC[0])-DBC[1])**2).mean()\n",
    "\n",
    "    #Calculating NBC Loss\n",
    "    sxx_NBC,syy_NBC,sxy_NBC = Stress(model,NBC[2])\n",
    "\n",
    "    sigma_2x2_NBC = torch.zeros(2,2).requires_grad_(True)\n",
    "    sigma_2x2_NBC = sigma_2x2_NBC.to(device)\n",
    "    sigma_2x2_NBC[0,0],sigma_2x2_NBC[0,1],sigma_2x2_NBC[1,0],sigma_2x2_NBC[1,1] = sxx_NBC,sxy_NBC,sxy_NBC,syy_NBC\n",
    "    #NBC Loss\n",
    "    NBC_loss = (((sigma_2x2_NBC @ NBC[1]) - NBC[0])**2).mean()\n",
    "\n",
    "\n",
    "    #Physics Loss\n",
    "    #Stress\n",
    "    sxx_phy,syy_phy,sxy_phy = Stress(model,batch)\n",
    "\n",
    "    #Divergence of Stress\n",
    "    dsxx = torch.autograd.grad(sxx_phy, batch, torch.ones_like(sxx_phy), retain_graph=True, create_graph=True)[0]\n",
    "    dsyy = torch.autograd.grad(syy_phy, batch, torch.ones_like(sxx_phy), retain_graph=True, create_graph=True)[0]\n",
    "    dsxy = torch.autograd.grad(sxy_phy, batch, torch.ones_like(sxx_phy), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    dsxxdx = dsxx[:,0]\n",
    "    dsyydy = dsyy[:,1]\n",
    "    dsxydx = dsxy[:,0]\n",
    "    dsxydy = dsxy[:,1]\n",
    "\n",
    "    div_stress = torch.zeros(2,len(dsxxdx)).requires_grad_(True)\n",
    "    div_stress = div_stress.to(device)\n",
    "    div_stress[0,:],div_stress[1,:] = dsxxdx + dsxydy, dsyydy + dsxydx\n",
    "    phy_loss = ((div_stress)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "    # calculate weighted loss\n",
    "    DBC_loss_weighted,NBC_loss_weighted = DBC_loss*DBC_weight , NBC_loss*NBC_weight\n",
    "\n",
    "    loss_weighted = DBC_loss_weighted + NBC_loss_weighted + phy_loss\n",
    "    loss_absolute = DBC_loss + NBC_loss + phy_loss\n",
    "\n",
    "\n",
    "\n",
    "    optim.zero_grad()\n",
    "    DBC_loss_weighted.backward(retain_graph=True)\n",
    "    temp = list()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            temp.append(torch.reshape(param.grad,(-1,)))\n",
    "    DBC_grads.append(torch.cat(temp))\n",
    "\n",
    "\n",
    "    optim.zero_grad()\n",
    "    NBC_loss_weighted.backward(retain_graph=True)\n",
    "    temp = list()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            temp.append(torch.reshape(param.grad,(-1,)))\n",
    "    NBC_grads.append(torch.cat(temp))\n",
    "\n",
    "\n",
    "    optim.zero_grad()\n",
    "    phy_loss.backward(retain_graph=True)\n",
    "    temp = list()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            temp.append(torch.reshape(param.grad,(-1,)))\n",
    "    phy_grads.append(torch.cat(temp))\n",
    "\n",
    "    return (loss_weighted, loss_absolute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss weights are dynamically updated using:\n",
    "\n",
    "$\\hat{\\lambda}^{(n)}_{ic} = \\frac{max \\{| {\\nabla_{\\theta}} \\mathcal{L}_{physics} |\\} } {| \\overline{ {\\nabla_{\\theta}} \\mathcal{L}_{ic} } |} $\n",
    "or\n",
    "$ \\frac{std \\{ {\\nabla_{\\theta}} \\mathcal{L}_{physics} \\} } {std \\{ {\\nabla_{\\theta}} \\mathcal{L}_{ic} \\} } $\n",
    "or\n",
    "$\\frac{|| {\\nabla_{\\theta}} \\mathcal{L}_{physics} ||_{2} } {|| {\\nabla_{\\theta}} \\mathcal{L}_{ic} ||_{2} } $\n",
    "\n",
    "$\\lambda^{(n)}_{ic} = \\alpha \\lambda^{(n-1)}_{ic} + (1 - \\alpha) \\hat{\\lambda}^{(n)}_{ic}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy_grads, NBC_grads, DBC_grads = list(), list(), list()\n",
    "NBC_weight, DBC_weight = 1.0, 1.0\n",
    "\n",
    "def weight_update_max_mean(w_update, a=0.9):\n",
    "    phy_loss_max = torch.max(torch.abs(torch.stack(phy_grads[-w_update:])))\n",
    "    \n",
    "    NBC_loss_mean = torch.mean(torch.abs(torch.stack(NBC_grads[-w_update:])))\n",
    "    NBC_weight_hat = (phy_loss_max / NBC_loss_mean).item()\n",
    "    new_NBC_weight = (1-a)*NBC_weight + a*NBC_weight_hat\n",
    "\n",
    "    DBC_loss_mean = torch.mean(torch.abs(torch.stack(DBC_grads[-w_update:])))\n",
    "    DBC_weight_hat = (phy_loss_max / DBC_loss_mean).item()\n",
    "    new_DBC_weight = (1-a)*DBC_weight + a*DBC_weight_hat\n",
    "\n",
    "    return new_NBC_weight, new_DBC_weight\n",
    "\n",
    "def weight_update_std(w_update, a=0.9):\n",
    "    phy_loss_std = torch.std(torch.stack(phy_grads[-w_update:]))\n",
    "\n",
    "    NBC_loss_std = torch.std(torch.stack(NBC_grads[-w_update:]))\n",
    "    NBC_weight_hat = (phy_loss_std / NBC_loss_std).item()\n",
    "    new_NBC_weight = (1-a)*NBC_weight + a*NBC_weight_hat\n",
    "\n",
    "    DBC_loss_std = torch.std(torch.stack(DBC_grads[-w_update:]))\n",
    "    DBC_weight_hat = (phy_loss_std / DBC_loss_std).item()\n",
    "    new_DBC_weight = (1-a)*DBC_weight + a*DBC_weight_hat\n",
    "\n",
    "    return new_NBC_weight, new_DBC_weight\n",
    "\n",
    "def weight_update_norm(w_update, a=0.9):\n",
    "    phy_loss_norm = torch.linalg.matrix_norm(torch.stack(phy_grads[-w_update:]))\n",
    "    \n",
    "    NBC_loss_norm = torch.linalg.matrix_norm(torch.stack(NBC_grads[-w_update:]))\n",
    "    NBC_weight_hat = (phy_loss_norm / NBC_loss_norm).item()\n",
    "    new_NBC_weight = (1-a)*NBC_weight + a*NBC_weight_hat\n",
    "\n",
    "    DBC_loss_norm = torch.linalg.matrix_norm(torch.stack(DBC_grads[-w_update:]))\n",
    "    DBC_weight_hat = (phy_loss_norm / DBC_loss_norm).item()\n",
    "    new_DBC_weight = (1-a)*DBC_weight + a*DBC_weight_hat\n",
    "\n",
    "    return new_NBC_weight, new_DBC_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_adam = {'epoch':-1}\n",
    "best_model_LBFGS = {'epoch':-1}\n",
    "\n",
    "def checkpoint(best_model):\n",
    "    if (best_model['epoch'] == -1) or (prev_loss[-1] < best_model['loss']):\n",
    "        best_model = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': copy.deepcopy(copy.deepcopy(model.state_dict())),\n",
    "            'optimizer_state_dict': copy.deepcopy(copy.deepcopy(optim.state_dict())),\n",
    "            'loss': prev_loss[-1],\n",
    "                    }\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(n=0):\n",
    "    plt.plot(np.log10(prev_loss[n:]),c='tab:blue',label='Log Loss')\n",
    "    plt.title(f\"Log Loss over {len(prev_loss[n:])} Calls\")\n",
    "    plt.xlabel(\"Calls\")\n",
    "    plt.ylabel(\"Log Loss\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(torch.log10(torch.mean(torch.abs(torch.stack(phy_grads[n:],1)),0)).cpu(),label='log mean phy loss grad',c='tab:green')\n",
    "    plt.title(f\"Gradient of parameters wrt Physics Loss over {len(prev_loss[n:])} Calls\")\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log Mean gradients for model parameters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(torch.log10(torch.mean(torch.abs(torch.stack(NBC_grads[n:],1)),0)).cpu(),label='log mean NBC loss grad',c='tab:orange')\n",
    "    plt.title(f\"Gradient of parameters wrt NBC Loss over {len(prev_loss[n:])} Calls\")\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log Mean gradients for model parameters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(torch.log10(torch.mean(torch.abs(torch.stack(DBC_grads[n:],1)),0)).cpu(),label='log mean DBC loss grad',c='tab:red')\n",
    "    plt.title(f\"Gradient of parameters wrt DBC Loss over {len(prev_loss[n:])} Calls\")\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log Mean gradients for model parameters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closure Fn for Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    loss_weighted, loss_absolute = loss_fn(model, train)\n",
    "    optim.zero_grad()\n",
    "    loss_weighted.backward()\n",
    "    prev_loss.append(loss_absolute.item())\n",
    "    return loss_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 2.00E+00\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PINNs\\2D Isotropic Beam.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m prev_loss \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs[\u001b[39m0\u001b[39m]):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optim\u001b[39m.\u001b[39mstep(closure)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     best_model_adam \u001b[39m=\u001b[39m checkpoint(best_model_adam)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m#if not (epoch%w_update): NBC_weight , DBC_weight = weight_update_norm(w_update)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 121\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    124\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PINNs\\2D Isotropic Beam.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     loss_weighted, loss_absolute \u001b[39m=\u001b[39m loss_fn(model, train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss_weighted\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PINNs\\2D Isotropic Beam.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m DBC_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mcat(temp))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m NBC_loss_weighted\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m temp \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Documents/GitHub/PINNs/2D%20Isotropic%20Beam.ipynb#X33sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mnamed_parameters():\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# train\n",
    "n_epochs = (3000,20)\n",
    "w_update = 1\n",
    "# start with Adam optimizer\n",
    "optim = Adam(model.parameters())\n",
    "prev_loss = list()\n",
    "\n",
    "for epoch in range(n_epochs[0]):\n",
    "    \n",
    "    optim.step(closure)\n",
    "    best_model_adam = checkpoint(best_model_adam)\n",
    "\n",
    "    #if not (epoch%w_update): NBC_weight , DBC_weight = weight_update_norm(w_update)\n",
    "    if not (epoch%1000): print(f'Finished epoch {epoch}, latest loss {prev_loss[-1]:.2E}')\n",
    "\n",
    "# plot for Adam optimization stage\n",
    "plot_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with L-BGFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change optim to L-BFGS\n",
    "optim = LBFGS(model.parameters())\n",
    "\n",
    "for epoch in range(n_epochs[0],sum(n_epochs)):\n",
    "    \n",
    "    optim.step(closure)\n",
    "    best_model_LBFGS = checkpoint(best_model_LBFGS)\n",
    "\n",
    "    #if not (epoch%w_update): ic_weight = weight_update_norm(w_update)\n",
    "    if not (epoch%10): print(f'Finished epoch {epoch}, latest loss {prev_loss[-1]:.2E}')\n",
    "\n",
    "plot_train(n_epochs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Best L-BFGS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_LBFGS['model_state_dict'])\n",
    "optim.load_state_dict(best_model_LBFGS['optimizer_state_dict'])\n",
    "epoch = best_model_LBFGS['epoch']\n",
    "loss = best_model_LBFGS['loss']\n",
    "model.eval()\n",
    "\n",
    "plot_comp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Best Adam Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_adam['model_state_dict'])\n",
    "optim.load_state_dict(best_model_adam['optimizer_state_dict'])\n",
    "epoch = best_model_adam['epoch']\n",
    "loss = best_model_adam['loss']\n",
    "model.eval()\n",
    "\n",
    "plot_comp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining wih best model using Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = LBFGS(model.parameters())\n",
    "\n",
    "for epoch in range(n_epochs[0],sum(n_epochs)):\n",
    "    \n",
    "    optim.step(closure)\n",
    "    best_model_LBFGS = checkpoint(best_model_LBFGS)\n",
    "\n",
    "    if not (epoch%w_update): ic_weight = weight_update_norm(w_update)\n",
    "    if not (epoch%10): print(f'Finished epoch {epoch}, latest loss {prev_loss[-1]:.2E}')\n",
    "\n",
    "plot_train(sum(n_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_LBFGS['model_state_dict'])\n",
    "optim.load_state_dict(best_model_LBFGS['optimizer_state_dict'])\n",
    "epoch = best_model_LBFGS['epoch']\n",
    "loss = best_model_LBFGS['loss']\n",
    "model.eval()\n",
    "\n",
    "plot_comp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(best_model['model_state_dict'])\n",
    "#optim.load_state_dict(best_model['optimizer_state_dict'])\n",
    "#epoch = best_model['epoch']\n",
    "#loss = best_model['loss']\n",
    "\n",
    "#PATH = f\"Models\\\\{loss:.2E}_{epoch}.tar\"\n",
    "#torch.save(best_model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"Models\\\\\"\n",
    "\n",
    "#checkpoint = torch.load(PATH, map_location=device)\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#epoch = checkpoint['epoch']\n",
    "#loss = checkpoint['loss']\n",
    "\n",
    "#model.eval()\n",
    "# - or -\n",
    "#model.train()\n",
    "\n",
    "#plot_comp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2bb01db43c92608afe528117be8348869a8bbfc2e823dee3781b388bf5fc44d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
