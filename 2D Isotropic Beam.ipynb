{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T03:13:33.844380Z",
     "iopub.status.busy": "2022-07-15T03:13:33.844026Z",
     "iopub.status.idle": "2022-07-15T03:13:33.853719Z",
     "shell.execute_reply": "2022-07-15T03:13:33.852749Z",
     "shell.execute_reply.started": "2022-07-15T03:13:33.844343Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim import LBFGS\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physical System to Model: 2D Deformation, Linear Isotropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBC, positions and displacments\n",
    "pos = [(0.0,0.0)]\n",
    "disp = [(0.0,0.0)]\n",
    "pos, disp = torch.tensor(pos).requires_grad_(True).to(device),torch.tensor(disp).requires_grad_(True).to(device)\n",
    "DBC = [pos,disp]\n",
    "\n",
    "#NBC, Scalar Traction Force, Normal Vector, and Position.\n",
    "T_hat = [(2.0,0)]\n",
    "n_hat = [(1.0,0)]\n",
    "T_pos = [(10.0,5.0)]\n",
    "T_hat, n_hat, T_pos = torch.tensor(T_hat).requires_grad_(True).t().to(device),torch.tensor(n_hat).requires_grad_(True).t().to(device),torch.tensor(T_pos).requires_grad_(True).to(device)\n",
    "NBC = [T_hat,n_hat,T_pos]\n",
    "\n",
    "\n",
    "# Physical Parameters, E = Youngs Modulus, v = Poisson's ratio.\n",
    "# Rubber\n",
    "p = dict(E=4.0, v=0.45)\n",
    "#Stiffness mat, 2D\n",
    "C = [[1, -p['v'], 0],\n",
    "     [-p['v'], 1, 0],\n",
    "     [0, 0, 1+p['v']]]\n",
    "C = torch.tensor(C).to(device)\n",
    "C = C/p['E']\n",
    "C = C.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating vector for Training Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "# Range of t to be modelled [0,10]\n",
    "combi = torch.linspace(0,m,20)\n",
    "train = torch.combinations(combi, with_replacement=True).requires_grad_(True)\n",
    "train = train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comp():\n",
    "    x_plot = np.linspace(0, m, 5)\n",
    "    y_plot = np.linspace(0, m, 5)\n",
    "\n",
    "    combi = torch.linspace(0,m,5).to(device)\n",
    "    plot_input = torch.combinations(combi, with_replacement=True).requires_grad_(True)\n",
    "    pred = model(plot_input)\n",
    "    \n",
    "    _, loss_absolute = loss_fn(model, plot_input)\n",
    "    pred_plot = pred.cpu().detach().numpy() + plot_input.cpu().detach().numpy()\n",
    "    \n",
    "    print(\"pred\")\n",
    "    print(pred.cpu().detach().numpy())\n",
    "    print(\"plotting\")\n",
    "    print(plot_input.cpu().detach().numpy())\n",
    "    print(\"sum\")\n",
    "    print(pred.cpu().detach().numpy() + plot_input.cpu().detach().numpy())\n",
    "\n",
    "    plt.plot(pred_plot[:,0].T,pred_plot[:,1].T,label='Deformed',marker='.')\n",
    "    plt.xlabel('Position')\n",
    "    plt.title(f\"Deformation, Loss: {loss_absolute:.2E}, Epoch {epoch}\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_num nodes, i_len nodes per layer and Tanh activation fn, wide and shallow (64x4)\n",
    "i_num = 2\n",
    "i_len = 64\n",
    "#params to take and approximate\n",
    "params_in = 2\n",
    "params_out = 2\n",
    "\n",
    "# fn to create model\n",
    "def pinn(input_size, output_size, num_layers, nodes_per_layer, activation):\n",
    "    layers = []\n",
    "    \n",
    "    # Input layer\n",
    "    layers.append(nn.Linear(input_size, nodes_per_layer))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for layer in range(num_layers):\n",
    "        layers.append(nn.Linear(nodes_per_layer, nodes_per_layer))\n",
    "        if layer == 0 and activation==Siren:\n",
    "            layers.append(activation(l=0))\n",
    "        else:\n",
    "            layers.append(activation())\n",
    "    \n",
    "    # Output layer\n",
    "    layers.append(nn.Linear(nodes_per_layer, output_size))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# custom trainable Tanh activation fn\n",
    "class TTanh(nn.Module):\n",
    "    def __init__(self, features=i_len):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.a = nn.Parameter(torch.randn(1, features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.a * x)\n",
    "\n",
    "#custom SIREN activation fn\n",
    "class Siren(nn.Module):\n",
    "    def __init__(self, features=i_len, l=1):\n",
    "        super(Siren, self).__init__()\n",
    "        self.features = features\n",
    "        # Learnable parameters for the phase and bias\n",
    "        if l: w = 1 \n",
    "        else: w = 30\n",
    "        n = torch.sqrt(torch.tensor(6.0/features))\n",
    "        self.a_weight = nn.Parameter((torch.rand(1, features) *2*n - n)*w)\n",
    "        self.a_bias = nn.Parameter(torch.randn(1, features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the periodic activation function\n",
    "        return torch.sin( self.a_weight*x + self.a_bias)\n",
    "\n",
    "# Model to be trained using only Adam\n",
    "model = pinn(params_in, params_out, i_num, i_len, TTanh)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function has the following components:\n",
    "\n",
    "Physics Loss:\n",
    "\n",
    "$\\mathcal{L}_{Phy} = \\frac{1}{N}\\sum[\\frac{\\partial \\sigma_{ij}}{\\partial x_{j}}]^2$\n",
    "\n",
    "NBC Loss:\n",
    "\n",
    "$\\mathcal{L}_{NBC} = \\frac{1}{N}\\sum[\\sigma.n - \\hat{T}]^2$\n",
    "\n",
    "DBC Loss:\n",
    "\n",
    "$\\mathcal{L}_{DBC} = \\frac{1}{N}\\sum[u-\\hat{u}]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stress(model,input):\n",
    "    # Strain from displacement field\n",
    "    pred = model(input)\n",
    "    \n",
    "    dux = torch.autograd.grad(pred[:,0], input, torch.ones_like(pred[:,0]), retain_graph=True, create_graph=True)[0]\n",
    "    duy = torch.autograd.grad(pred[:,1], input, torch.ones_like(pred[:,1]), retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "    duxdx = dux[:,0]\n",
    "    duxdy = dux[:,1]\n",
    "    duydx = duy[:,0]\n",
    "    duydy = duy[:,1]\n",
    "\n",
    "    epsilon = torch.stack((duxdx, duydy, 0.5 * (duydx + duxdy)))\n",
    "    \n",
    "    # Stress from Strain and Constitutive matrix\n",
    "    sigma_3xn = C @ epsilon\n",
    "    \n",
    "    return sigma_3xn[0,:],sigma_3xn[1,:],sigma_3xn[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, batch):\n",
    "    # model: model\n",
    "    # batch: vector with x\n",
    "\n",
    "    # Calulate loss from initial conditions (DBC)\n",
    "    DBC_loss = ((model(DBC[0])-DBC[1])**2).mean()\n",
    "\n",
    "    #Calculating NBC Loss\n",
    "    sxx_NBC,syy_NBC,sxy_NBC = Stress(model,NBC[2])\n",
    "\n",
    "    sigma_1x2_x_NBC = torch.stack((sxx_NBC,sxy_NBC.clone()), dim=1)\n",
    "    sigma_1x2_y_NBC = torch.stack((sxy_NBC.clone(),syy_NBC), dim=1)\n",
    "    sigma_2x2_NBC = torch.cat((sigma_1x2_x_NBC, sigma_1x2_y_NBC), dim=0)\n",
    "\n",
    "    #NBC Loss\n",
    "    NBC_loss = (((sigma_2x2_NBC @ NBC[1]) - NBC[0])**2).mean()\n",
    "\n",
    "\n",
    "    #Physics Loss\n",
    "    #Stress\n",
    "    sxx_phy,syy_phy,sxy_phy = Stress(model,batch)\n",
    "\n",
    "    #Divergence of Stress\n",
    "    dsxx = torch.autograd.grad(sxx_phy, batch, torch.ones_like(sxx_phy), retain_graph=True, create_graph=True)[0]\n",
    "    dsyy = torch.autograd.grad(syy_phy, batch, torch.ones_like(sxx_phy), retain_graph=True, create_graph=True)[0]\n",
    "    dsxy = torch.autograd.grad(sxy_phy, batch, torch.ones_like(sxx_phy), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    dsxxdx = dsxx[:,0]\n",
    "    dsyydy = dsyy[:,1]\n",
    "    dsxydx = dsxy[:,0]\n",
    "    dsxydy = dsxy[:,1]\n",
    "\n",
    "    div_stress = torch.stack((dsxxdx + dsxydy, dsyydy + dsxydx), dim=0)\n",
    "    phy_loss = ((div_stress)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "    # calculate weighted loss\n",
    "    DBC_loss_weighted,NBC_loss_weighted = DBC_loss*DBC_weight , NBC_loss*NBC_weight\n",
    "\n",
    "    obtain_grads([DBC_loss_weighted, NBC_loss_weighted, phy_loss])\n",
    "\n",
    "    loss_weighted = DBC_loss_weighted + NBC_loss_weighted + phy_loss\n",
    "    loss_absolute = DBC_loss + NBC_loss + phy_loss\n",
    "\n",
    "    return loss_weighted, loss_absolute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss weights are dynamically updated using:\n",
    "\n",
    "$\\hat{\\lambda}^{(n)}_{ic} = \\frac{max \\{| {\\nabla_{\\theta}} \\mathcal{L}_{physics} |\\} } {| \\overline{ {\\nabla_{\\theta}} \\mathcal{L}_{ic} } |} $\n",
    "or\n",
    "$ \\frac{std \\{ {\\nabla_{\\theta}} \\mathcal{L}_{physics} \\} } {std \\{ {\\nabla_{\\theta}} \\mathcal{L}_{ic} \\} } $\n",
    "or\n",
    "$\\frac{|| {\\nabla_{\\theta}} \\mathcal{L}_{physics} ||_{2} } {|| {\\nabla_{\\theta}} \\mathcal{L}_{ic} ||_{2} } $\n",
    "\n",
    "$\\lambda^{(n)}_{ic} = \\alpha \\lambda^{(n-1)}_{ic} + (1 - \\alpha) \\hat{\\lambda}^{(n)}_{ic}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy_grads, NBC_grads, DBC_grads = list(), list(), list()\n",
    "list_of_lists = [DBC_grads, NBC_grads, phy_grads]\n",
    "NBC_weight, DBC_weight = 1.0, 1.0\n",
    "\n",
    "def weight_update_max_mean(w_update, a=0.9):\n",
    "    phy_loss_max = torch.max(torch.abs(torch.stack(phy_grads[-w_update:])))\n",
    "    \n",
    "    NBC_loss_mean = torch.mean(torch.abs(torch.stack(NBC_grads[-w_update:])))\n",
    "    NBC_weight_hat = (phy_loss_max / NBC_loss_mean).item()\n",
    "    new_NBC_weight = (1-a)*NBC_weight + a*NBC_weight_hat\n",
    "\n",
    "    DBC_loss_mean = torch.mean(torch.abs(torch.stack(DBC_grads[-w_update:])))\n",
    "    DBC_weight_hat = (phy_loss_max / DBC_loss_mean).item()\n",
    "    new_DBC_weight = (1-a)*DBC_weight + a*DBC_weight_hat\n",
    "\n",
    "    return new_NBC_weight, new_DBC_weight\n",
    "\n",
    "def weight_update_std(w_update, a=0.9):\n",
    "    phy_loss_std = torch.std(torch.stack(phy_grads[-w_update:]))\n",
    "\n",
    "    NBC_loss_std = torch.std(torch.stack(NBC_grads[-w_update:]))\n",
    "    NBC_weight_hat = (phy_loss_std / NBC_loss_std).item()\n",
    "    new_NBC_weight = (1-a)*NBC_weight + a*NBC_weight_hat\n",
    "\n",
    "    DBC_loss_std = torch.std(torch.stack(DBC_grads[-w_update:]))\n",
    "    DBC_weight_hat = (phy_loss_std / DBC_loss_std).item()\n",
    "    new_DBC_weight = (1-a)*DBC_weight + a*DBC_weight_hat\n",
    "\n",
    "    return new_NBC_weight, new_DBC_weight\n",
    "\n",
    "def weight_update_norm(w_update, a=0.9):\n",
    "    phy_loss_norm = torch.linalg.matrix_norm(torch.stack(phy_grads[-w_update:]))\n",
    "    \n",
    "    NBC_loss_norm = torch.linalg.matrix_norm(torch.stack(NBC_grads[-w_update:]))\n",
    "    NBC_weight_hat = (phy_loss_norm / NBC_loss_norm).item()\n",
    "    new_NBC_weight = (1-a)*NBC_weight + a*NBC_weight_hat\n",
    "\n",
    "    DBC_loss_norm = torch.linalg.matrix_norm(torch.stack(DBC_grads[-w_update:]))\n",
    "    DBC_weight_hat = (phy_loss_norm / DBC_loss_norm).item()\n",
    "    new_DBC_weight = (1-a)*DBC_weight + a*DBC_weight_hat\n",
    "\n",
    "    return new_NBC_weight, new_DBC_weight\n",
    "\n",
    "def obtain_grads(losses):\n",
    "\n",
    "    for i in range(len(list_of_lists)):\n",
    "        optim.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "        temp = list()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                temp.append(torch.reshape(param.grad,(-1,)))\n",
    "        list_of_lists[i].append(torch.cat(temp))\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_adam = {'epoch':-1}\n",
    "best_model_LBFGS = {'epoch':-1}\n",
    "\n",
    "def checkpoint(best_model):\n",
    "    if (best_model['epoch'] == -1) or (prev_loss[-1] < best_model['loss']):\n",
    "        best_model = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': copy.deepcopy(copy.deepcopy(model.state_dict())),\n",
    "            'optimizer_state_dict': copy.deepcopy(copy.deepcopy(optim.state_dict())),\n",
    "            'loss': prev_loss[-1],\n",
    "                    }\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(n=0):\n",
    "    plt.plot(np.log10(prev_loss[n:]),c='tab:blue',label='Log Loss')\n",
    "    plt.title(f\"Log Loss over {len(prev_loss[n:])} Calls\")\n",
    "    plt.xlabel(\"Calls\")\n",
    "    plt.ylabel(\"Log Loss\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(torch.log10(torch.mean(torch.abs(torch.stack(phy_grads[n:],1)),0)).cpu(),label='log mean phy loss grad',c='tab:green')\n",
    "    plt.title(f\"Gradient of parameters wrt Physics Loss over {len(prev_loss[n:])} Calls\")\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log Mean gradients for model parameters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(torch.log10(torch.mean(torch.abs(torch.stack(NBC_grads[n:],1)),0)).cpu(),label='log mean NBC loss grad',c='tab:orange')\n",
    "    plt.title(f\"Gradient of parameters wrt NBC Loss over {len(prev_loss[n:])} Calls\")\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log Mean gradients for model parameters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(torch.log10(torch.mean(torch.abs(torch.stack(DBC_grads[n:],1)),0)).cpu(),label='log mean DBC loss grad',c='tab:red')\n",
    "    plt.title(f\"Gradient of parameters wrt DBC Loss over {len(prev_loss[n:])} Calls\")\n",
    "    plt.xlabel(\"Loss Computations\")\n",
    "    plt.ylabel(\"Log Mean gradients for model parameters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closure Fn for Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    loss_weighted, loss_absolute = loss_fn(model, train)\n",
    "    optim.zero_grad()\n",
    "    loss_weighted.backward()\n",
    "    prev_loss.append(loss_absolute.item())\n",
    "    return loss_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "n_epochs = (3000,20)\n",
    "w_update = 1\n",
    "# start with Adam optimizer\n",
    "optim = Adam(model.parameters())\n",
    "prev_loss = list()\n",
    "\n",
    "for epoch in range(n_epochs[0]):\n",
    "    \n",
    "    optim.step(closure)\n",
    "    best_model_adam = checkpoint(best_model_adam)\n",
    "\n",
    "    #if not (epoch%w_update): NBC_weight , DBC_weight = weight_update_norm(w_update)\n",
    "    if not (epoch%1000): print(f'Finished epoch {epoch}, latest loss {prev_loss[-1]:.2E}')\n",
    "\n",
    "# plot for Adam optimization stage\n",
    "plot_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with L-BGFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change optim to L-BFGS\n",
    "optim = LBFGS(model.parameters())\n",
    "\n",
    "for epoch in range(n_epochs[0],sum(n_epochs)):\n",
    "    \n",
    "    optim.step(closure)\n",
    "    best_model_LBFGS = checkpoint(best_model_LBFGS)\n",
    "\n",
    "    #if not (epoch%w_update): ic_weight = weight_update_norm(w_update)\n",
    "    if not (epoch%10): print(f'Finished epoch {epoch}, latest loss {prev_loss[-1]:.2E}')\n",
    "\n",
    "plot_train(n_epochs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Best L-BFGS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_LBFGS['model_state_dict'])\n",
    "optim.load_state_dict(best_model_LBFGS['optimizer_state_dict'])\n",
    "epoch = best_model_LBFGS['epoch']\n",
    "loss = best_model_LBFGS['loss']\n",
    "model.eval()\n",
    "\n",
    "plot_comp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Best Adam Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_adam['model_state_dict'])\n",
    "optim.load_state_dict(best_model_adam['optimizer_state_dict'])\n",
    "epoch = best_model_adam['epoch']\n",
    "loss = best_model_adam['loss']\n",
    "model.eval()\n",
    "\n",
    "plot_comp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining wih best model using Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = LBFGS(model.parameters())\n",
    "\n",
    "for epoch in range(n_epochs[0],sum(n_epochs)):\n",
    "    \n",
    "    optim.step(closure)\n",
    "    best_model_LBFGS = checkpoint(best_model_LBFGS)\n",
    "\n",
    "    if not (epoch%w_update): ic_weight = weight_update_norm(w_update)\n",
    "    if not (epoch%10): print(f'Finished epoch {epoch}, latest loss {prev_loss[-1]:.2E}')\n",
    "\n",
    "plot_train(sum(n_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_LBFGS['model_state_dict'])\n",
    "optim.load_state_dict(best_model_LBFGS['optimizer_state_dict'])\n",
    "epoch = best_model_LBFGS['epoch']\n",
    "loss = best_model_LBFGS['loss']\n",
    "model.eval()\n",
    "\n",
    "plot_comp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(best_model['model_state_dict'])\n",
    "#optim.load_state_dict(best_model['optimizer_state_dict'])\n",
    "#epoch = best_model['epoch']\n",
    "#loss = best_model['loss']\n",
    "\n",
    "#PATH = f\"Models\\\\{loss:.2E}_{epoch}.tar\"\n",
    "#torch.save(best_model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"Models\\\\\"\n",
    "\n",
    "#checkpoint = torch.load(PATH, map_location=device)\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#epoch = checkpoint['epoch']\n",
    "#loss = checkpoint['loss']\n",
    "\n",
    "#model.eval()\n",
    "# - or -\n",
    "#model.train()\n",
    "\n",
    "#plot_comp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2bb01db43c92608afe528117be8348869a8bbfc2e823dee3781b388bf5fc44d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
